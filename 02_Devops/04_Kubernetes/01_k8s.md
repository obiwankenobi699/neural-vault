---
title:
  "{ Title }":
tags:
  - DevOps
  - k8s
created:
  "{ date }":
updated:
  "{ date }":
---
# Kubernetes Architecture - Complete DevOps Reference

```
╔════════════════════════════════════════════════════════════════╗
║                                                                ║
║   ██╗  ██╗██╗   ██╗██████╗ ███████╗██████╗ ███╗   ██╗███████╗ ║
║   ██║ ██╔╝██║   ██║██╔══██╗██╔════╝██╔══██╗████╗  ██║██╔════╝ ║
║   █████╔╝ ██║   ██║██████╔╝█████╗  ██████╔╝██╔██╗ ██║█████╗   ║
║   ██╔═██╗ ██║   ██║██╔══██╗██╔══╝  ██╔══██╗██║╚██╗██║██╔══╝   ║
║   ██║  ██╗╚██████╔╝██████╔╝███████╗██║  ██║██║ ╚████║███████╗ ║
║   ╚═╝  ╚═╝ ╚═════╝ ╚═════╝ ╚══════╝╚═╝  ╚═╝╚═╝  ╚═══╝╚══════╝ ║
║                                                                ║
║          Container Orchestration Platform                     ║
║                                                                ║
╚════════════════════════════════════════════════════════════════╝
```

---

## Table of Contents

1. [Evolution and History](https://claude.ai/chat/ccc747c5-c7ba-45c1-8ef0-6f9e4b64f666#evolution-and-history)
2. [Architecture Paradigms](https://claude.ai/chat/ccc747c5-c7ba-45c1-8ef0-6f9e4b64f666#architecture-paradigms)
3. [Configuration Management](https://claude.ai/chat/ccc747c5-c7ba-45c1-8ef0-6f9e4b64f666#configuration-management)
4. [Kubernetes Architecture Overview](https://claude.ai/chat/ccc747c5-c7ba-45c1-8ef0-6f9e4b64f666#kubernetes-architecture-overview)
5. [Control Plane Components](https://claude.ai/chat/ccc747c5-c7ba-45c1-8ef0-6f9e4b64f666#control-plane-components)
6. [Worker Node Components](https://claude.ai/chat/ccc747c5-c7ba-45c1-8ef0-6f9e4b64f666#worker-node-components)
7. [Core Objects](https://claude.ai/chat/ccc747c5-c7ba-45c1-8ef0-6f9e4b64f666#core-objects)
8. [Networking Architecture](https://claude.ai/chat/ccc747c5-c7ba-45c1-8ef0-6f9e4b64f666#networking-architecture)
9. [Storage Architecture](https://claude.ai/chat/ccc747c5-c7ba-45c1-8ef0-6f9e4b64f666#storage-architecture)
10. [Interview Preparation](https://claude.ai/chat/ccc747c5-c7ba-45c1-8ef0-6f9e4b64f666#interview-preparation)

---

## Evolution and History

### From Bare Metal to Containers

```ascii
┌────────────────────────────────────────────────────────────┐
│              INFRASTRUCTURE EVOLUTION                      │
├────────────────────────────────────────────────────────────┤
│                                                            │
│  Era 1: Physical Servers (1990s-2000s)                    │
│  ┌──────────────────────────────────────────────────┐     │
│  │  Physical Server                                 │     │
│  │  ┌────────────────────────────────────────┐     │     │
│  │  │  Application A    Application B        │     │     │
│  │  │  Dependencies     Dependencies         │     │     │
│  │  │  ────────────────────────────────────  │     │     │
│  │  │         Operating System               │     │     │
│  │  │         Hardware Resources             │     │     │
│  │  └────────────────────────────────────────┘     │     │
│  └──────────────────────────────────────────────────┘     │
│  Issues: Resource waste, no isolation, slow deployment    │
│                                                            │
│  Era 2: Virtualization (2000s-2010s)                      │
│  ┌──────────────────────────────────────────────────┐     │
│  │  Physical Server                                 │     │
│  │  ┌───────────┐  ┌───────────┐  ┌───────────┐   │     │
│  │  │    VM 1   │  │    VM 2   │  │    VM 3   │   │     │
│  │  │  App A    │  │  App B    │  │  App C    │   │     │
│  │  │  Guest OS │  │  Guest OS │  │  Guest OS │   │     │
│  │  └───────────┘  └───────────┘  └───────────┘   │     │
│  │  ┌────────────────────────────────────────┐     │     │
│  │  │         Hypervisor (VMware/KVM)        │     │     │
│  │  │         Host Operating System          │     │     │
│  │  │         Physical Hardware              │     │     │
│  │  └────────────────────────────────────────┘     │     │
│  └──────────────────────────────────────────────────┘     │
│  Benefits: Isolation, better resource use                  │
│  Issues: Heavy overhead, slow boot, large image sizes     │
│                                                            │
│  Era 3: Containers (2010s-Present)                        │
│  ┌──────────────────────────────────────────────────┐     │
│  │  Physical/Virtual Server                         │     │
│  │  ┌─────────┐  ┌─────────┐  ┌─────────┐         │     │
│  │  │Container│  │Container│  │Container│         │     │
│  │  │  App A  │  │  App B  │  │  App C  │         │     │
│  │  │  Libs   │  │  Libs   │  │  Libs   │         │     │
│  │  └─────────┘  └─────────┘  └─────────┘         │     │
│  │  ┌────────────────────────────────────────┐     │     │
│  │  │    Container Runtime (Docker)          │     │     │
│  │  │    Host Operating System               │     │     │
│  │  │    Physical/Virtual Hardware           │     │     │
│  │  └────────────────────────────────────────┘     │     │
│  └──────────────────────────────────────────────────┘     │
│  Benefits: Lightweight, fast, portable, efficient         │
│                                                            │
└────────────────────────────────────────────────────────────┘
```

### Google Borg: The Predecessor

```markmap
# Google Borg History
## Timeline
### 2003: Borg development begins
### 2004: Borg production deployment
### 2013: Borg paper published
### 2014: Kubernetes announced
### 2015: Kubernetes v1.0 released
## Borg Features
### Cluster management
### Resource scheduling
### Container orchestration
### Service discovery
### Load balancing
### Auto-scaling
## Borg to Kubernetes
### Open source vs proprietary
### Simpler architecture
### Declarative API
### Cloud-native design
### Community-driven
## Key Learnings
### Lessons from 10+ years at Google
### Billions of containers per week
### Large-scale cluster management
### Fault tolerance patterns
### Resource optimization
```

### Borg Architecture vs Kubernetes

```mermaid
graph TB
    subgraph "Google Borg (Internal)"
        A1[BorgMaster] --> A2[Scheduler]
        A1 --> A3[Borglet]
        A3 --> A4[Tasks]
        A5[BNS<br/>Borg Name Service] --> A3
    end
    
    subgraph "Kubernetes (Open Source)"
        B1[API Server] --> B2[Scheduler]
        B1 --> B3[Kubelet]
        B3 --> B4[Pods]
        B5[CoreDNS<br/>Service Discovery] --> B3
        B1 --> B6[etcd]
    end
    
    style A1 fill:#e74c3c
    style B1 fill:#2ecc71
```

### Why Kubernetes Exists

```ascii
╔════════════════════════════════════════════════════════════╗
║              THE ORCHESTRATION PROBLEM                     ║
╠════════════════════════════════════════════════════════════╣
║                                                            ║
║  Without Orchestration:                                   ║
║  ❌ Manual container placement                             ║
║  ❌ No automatic restart on failure                        ║
║  ❌ Manual scaling (up/down)                               ║
║  ❌ Complex networking setup                               ║
║  ❌ No service discovery                                   ║
║  ❌ Manual load balancing                                  ║
║  ❌ No rolling updates                                     ║
║  ❌ Configuration management chaos                         ║
║                                                            ║
║  With Kubernetes:                                         ║
║  ✓ Automated scheduling and placement                     ║
║  ✓ Self-healing (auto-restart)                            ║
║  ✓ Horizontal auto-scaling                                ║
║  ✓ Built-in service discovery                             ║
║  ✓ Integrated load balancing                              ║
║  ✓ Zero-downtime deployments                              ║
║  ✓ Declarative configuration                              ║
║  ✓ Secret and config management                           ║
║                                                            ║
╚════════════════════════════════════════════════════════════╝
```

---

## Architecture Paradigms

### Monolithic vs Microservices

#### Monolithic Architecture

```
┌─────────────────────────────────────────────────────────────┐
│                   MONOLITHIC APPLICATION                    │
│                                                             │
│  ┌───────────────────────────────────────────────────────┐ │
│  │                                                       │ │
│  │                Application Codebase                   │ │
│  │                                                       │ │
│  │  ┌──────────────┐  ┌──────────────┐  ┌───────────┐  │ │
│  │  │     UI       │  │   Business   │  │   Data    │  │ │
│  │  │   Layer      │◄─┤    Logic     │◄─┤  Access   │  │ │
│  │  └──────────────┘  └──────────────┘  └───────────┘  │ │
│  │                                                       │ │
│  │  All components tightly coupled                       │ │
│  │  Single deployment unit                               │ │
│  │  Shared database                                      │ │
│  │                                                       │ │
│  └───────────────────────────────────────────────────────┘ │
│                            │                                │
│                            ▼                                │
│                  ┌──────────────────┐                      │
│                  │  Single Database │                      │
│                  └──────────────────┘                      │
└─────────────────────────────────────────────────────────────┘

Characteristics:
✓ Simple to develop initially
✓ Simple to deploy (single unit)
✓ Simple to test (everything together)

❌ Difficult to scale (scale everything)
❌ Technology lock-in (one stack)
❌ Long deployment cycles
❌ Single point of failure
❌ Hard to maintain as it grows
❌ Team coordination overhead
```

#### Microservices Architecture

```
┌─────────────────────────────────────────────────────────────┐
│              MICROSERVICES APPLICATION                      │
│                                                             │
│  ┌──────────────┐   ┌──────────────┐   ┌──────────────┐   │
│  │   User       │   │   Product    │   │   Order      │   │
│  │   Service    │   │   Service    │   │   Service    │   │
│  │              │   │              │   │              │   │
│  │   Node.js    │   │   Python     │   │   Java       │   │
│  └──────┬───────┘   └──────┬───────┘   └──────┬───────┘   │
│         │                  │                  │            │
│         ▼                  ▼                  ▼            │
│  ┌──────────────┐   ┌──────────────┐   ┌──────────────┐   │
│  │   User DB    │   │  Product DB  │   │   Order DB   │   │
│  │  (Postgres)  │   │   (MongoDB)  │   │   (MySQL)    │   │
│  └──────────────┘   └──────────────┘   └──────────────┘   │
│                                                             │
│  ┌──────────────┐   ┌──────────────┐   ┌──────────────┐   │
│  │  Payment     │   │ Notification │   │  Analytics   │   │
│  │  Service     │   │   Service    │   │   Service    │   │
│  │              │   │              │   │              │   │
│  │   Go         │   │   Python     │   │   Scala      │   │
│  └──────┬───────┘   └──────┬───────┘   └──────┬───────┘   │
│         │                  │                  │            │
│         ▼                  ▼                  ▼            │
│  ┌──────────────┐   ┌──────────────┐   ┌──────────────┐   │
│  │  Payment DB  │   │  Message Q   │   │ Analytics DB │   │
│  │  (Postgres)  │   │  (RabbitMQ)  │   │ (Cassandra)  │   │
│  └──────────────┘   └──────────────┘   └──────────────┘   │
│                                                             │
│         All services communicate via API Gateway           │
│              and service mesh (if used)                    │
└─────────────────────────────────────────────────────────────┘

Characteristics:
✓ Independent scaling
✓ Technology flexibility
✓ Independent deployment
✓ Fault isolation
✓ Team autonomy

❌ Complex infrastructure
❌ Distributed system challenges
❌ Network latency
❌ Requires orchestration (Kubernetes!)
```

### Comparison Matrix

|Aspect|Monolithic|Microservices|
|---|---|---|
|**Architecture**|Single unified codebase|Multiple independent services|
|**Deployment**|All at once|Service by service|
|**Scaling**|Scale entire app|Scale individual services|
|**Technology**|Single stack|Polyglot (multiple stacks)|
|**Team Structure**|Single large team|Multiple small teams|
|**Development Speed**|Fast initially, slow later|Slow initially, fast later|
|**Testing**|Simple integration testing|Complex distributed testing|
|**Failure Impact**|Entire system down|Isolated to service|
|**Data Management**|Single database|Database per service|
|**Best For**|Small apps, startups|Large apps, enterprises|

### Microservices Communication Patterns

```mermaid
graph LR
    subgraph "Synchronous Communication"
        A[User Service] -->|REST/gRPC| B[Order Service]
        B -->|REST/gRPC| C[Payment Service]
    end
    
    subgraph "Asynchronous Communication"
        D[Order Service] -->|Event| E[Message Queue]
        E -->|Event| F[Notification Service]
        E -->|Event| G[Analytics Service]
    end
    
    subgraph "Service Mesh"
        H[Service A] <-->|Sidecar Proxy| I[Envoy]
        I <-->|mTLS| J[Envoy]
        J <-->|Sidecar Proxy| K[Service B]
    end
    
    style E fill:#f39c12
    style I fill:#3498db
    style J fill:#3498db
```

---

## Configuration Management

### Mutable vs Immutable Infrastructure

#### Mutable Infrastructure (Traditional)

```
┌─────────────────────────────────────────────────────────────┐
│              MUTABLE INFRASTRUCTURE                         │
│                   (Configuration Drift)                     │
├─────────────────────────────────────────────────────────────┤
│                                                             │
│  Initial State (Day 1):                                     │
│  ┌──────────────────────────────────────────────────┐      │
│  │  Server A                                        │      │
│  │  - Ubuntu 20.04                                  │      │
│  │  - Apache 2.4.41                                 │      │
│  │  - PHP 7.4                                       │      │
│  │  - App v1.0                                      │      │
│  └──────────────────────────────────────────────────┘      │
│                                                             │
│  After Manual Changes (Day 30):                             │
│  ┌──────────────────────────────────────────────────┐      │
│  │  Server A                                        │      │
│  │  - Ubuntu 20.04 (security patches applied)      │      │
│  │  - Apache 2.4.43 (manual upgrade)               │      │
│  │  - PHP 7.4.3 (hotfix applied)                   │      │
│  │  - App v1.2 (direct deployment)                 │      │
│  │  - Debug tools installed                        │      │
│  │  - Config files modified                        │      │
│  └──────────────────────────────────────────────────┘      │
│                                                             │
│  Problems:                                                  │
│  ❌ Configuration drift                                     │
│  ❌ "Works on my machine" syndrome                          │
│  ❌ Difficult to reproduce                                  │
│  ❌ Snowflake servers                                       │
│  ❌ Hard to audit changes                                   │
│  ❌ Rollback complexity                                     │
│                                                             │
│  Tools: Chef, Puppet, Ansible, Salt                        │
│  Approach: Modify servers in place                         │
│                                                             │
└─────────────────────────────────────────────────────────────┘
```

#### Immutable Infrastructure (Modern)

```
┌─────────────────────────────────────────────────────────────┐
│             IMMUTABLE INFRASTRUCTURE                        │
│              (No Configuration Drift)                       │
├─────────────────────────────────────────────────────────────┤
│                                                             │
│  Version 1.0:                                               │
│  ┌──────────────────────────────────────────────────┐      │
│  │  Container Image v1.0                            │      │
│  │  - Base: Ubuntu 20.04                            │      │
│  │  - Apache 2.4.41                                 │      │
│  │  - PHP 7.4                                       │      │
│  │  - App v1.0                                      │      │
│  │  ────────────────────────────────────────────    │      │
│  │  SHA256: abc123...                               │      │
│  │  Immutable - Never Modified                      │      │
│  └──────────────────────────────────────────────────┘      │
│                    │                                        │
│                    ▼ Build new image                        │
│  Version 1.2:                                               │
│  ┌──────────────────────────────────────────────────┐      │
│  │  Container Image v1.2                            │      │
│  │  - Base: Ubuntu 20.04                            │      │
│  │  - Apache 2.4.43                                 │      │
│  │  - PHP 7.4.3                                     │      │
│  │  - App v1.2                                      │      │
│  │  ────────────────────────────────────────────    │      │
│  │  SHA256: def456...                               │      │
│  │  Immutable - Never Modified                      │      │
│  └──────────────────────────────────────────────────┘      │
│                                                             │
│  Deployment Strategy:                                       │
│  1. Build new container image                               │
│  2. Deploy new containers                                   │
│  3. Terminate old containers                                │
│  4. Rollback = Deploy old image                             │
│                                                             │
│  Benefits:                                                  │
│  ✓ No configuration drift                                   │
│  ✓ Perfect reproducibility                                  │
│  ✓ Easy rollback                                            │
│  ✓ Version control for infrastructure                       │
│  ✓ Consistent environments                                  │
│  ✓ Simple audit trail                                       │
│                                                             │
│  Tools: Docker, Kubernetes, Terraform                       │
│  Approach: Replace, don't modify                            │
│                                                             │
└─────────────────────────────────────────────────────────────┘
```

### Configuration Management Evolution

```mermaid
timeline
    title Infrastructure Configuration Evolution
    2000-2005 : Manual Configuration
              : SSH into servers
              : Manual package installation
              : Shell scripts
    2005-2010 : Configuration Management
              : Chef, Puppet, Ansible
              : Infrastructure as Code
              : Mutable infrastructure
    2010-2015 : Virtualization & Cloud
              : AWS, Azure, GCP
              : VM templates
              : Auto-scaling groups
    2015-2020 : Containerization
              : Docker containers
              : Immutable infrastructure
              : Microservices
    2020-Present : Orchestration
                 : Kubernetes
                 : Service mesh
                 : GitOps
```

---

## Kubernetes Architecture Overview

### High-Level Architecture

```
┌──────────────────────────────────────────────────────────────────┐
│                      KUBERNETES CLUSTER                          │
├──────────────────────────────────────────────────────────────────┤
│                                                                  │
│  ┌────────────────────────────────────────────────────────────┐ │
│  │                   CONTROL PLANE                            │ │
│  │                   (Master Node)                            │ │
│  │                                                            │ │
│  │  ┌──────────────┐  ┌──────────────┐  ┌──────────────┐   │ │
│  │  │  API Server  │  │  Scheduler   │  │  Controller  │   │ │
│  │  │   (Port 443) │  │              │  │   Manager    │   │ │
│  │  │              │  │              │  │              │   │ │
│  │  └──────┬───────┘  └──────────────┘  └──────────────┘   │ │
│  │         │                                                 │ │
│  │         │           ┌──────────────┐                     │ │
│  │         └──────────►│    etcd      │                     │ │
│  │                     │ (Key-Value)  │                     │ │
│  │                     │    Store     │                     │ │
│  │                     └──────────────┘                     │ │
│  │                                                            │ │
│  │  Cluster State Management & Decision Making              │ │
│  └────────────────────────────────────────────────────────────┘ │
│                                │                                 │
│                                │ kubectl                         │
│                                │ RESTful API                     │
│                                ▼                                 │
│  ┌────────────────────────────────────────────────────────────┐ │
│  │                     WORKER NODES                           │ │
│  │                                                            │ │
│  │  ┌──────────────────────────┐  ┌──────────────────────┐  │ │
│  │  │   Worker Node 1          │  │   Worker Node 2      │  │ │
│  │  │                          │  │                      │  │ │
│  │  │  ┌────────────────────┐  │  │  ┌────────────────┐ │  │ │
│  │  │  │   Kubelet          │  │  │  │   Kubelet      │ │  │ │
│  │  │  │   (Node Agent)     │  │  │  │   (Node Agent) │ │  │ │
│  │  │  └────────────────────┘  │  │  └────────────────┘ │  │ │
│  │  │  ┌────────────────────┐  │  │  ┌────────────────┐ │  │ │
│  │  │  │   Kube-Proxy       │  │  │  │   Kube-Proxy   │ │  │ │
│  │  │  │   (Network Proxy)  │  │  │  │   (Network)    │ │  │ │
│  │  │  └────────────────────┘  │  │  └────────────────┘ │  │ │
│  │  │  ┌────────────────────┐  │  │  ┌────────────────┐ │  │ │
│  │  │  │ Container Runtime  │  │  │  │ Container      │ │  │ │
│  │  │  │ (containerd/CRI-O) │  │  │  │ Runtime        │ │  │ │
│  │  │  └────────────────────┘  │  │  └────────────────┘ │  │ │
│  │  │           │               │  │          │         │  │ │
│  │  │           ▼               │  │          ▼         │  │ │
│  │  │  ┌─────────────────────┐ │  │  ┌───────────────┐ │  │ │
│  │  │  │ POD   POD   POD     │ │  │  │ POD   POD     │ │  │ │
│  │  │  │ ┌───┐ ┌───┐ ┌───┐   │ │  │  │ ┌───┐ ┌───┐   │ │  │ │
│  │  │  │ │ C │ │ C │ │ C │   │ │  │  │ │ C │ │ C │   │ │  │ │
│  │  │  │ └───┘ └───┘ └───┘   │ │  │  │ └───┘ └───┘   │ │  │ │
│  │  │  └─────────────────────┘ │  │  └───────────────┘ │  │ │
│  │  │                          │  │                      │  │ │
│  │  └──────────────────────────┘  └──────────────────────┘  │ │
│  │                                                            │ │
│  │  Actual workload execution - Runs application containers  │ │
│  └────────────────────────────────────────────────────────────┘ │
│                                                                  │
└──────────────────────────────────────────────────────────────────┘

Legend: C = Container
```

### Declarative Model

```ascii
┌────────────────────────────────────────────────────────────┐
│             DECLARATIVE vs IMPERATIVE                      │
├────────────────────────────────────────────────────────────┤
│                                                            │
│  IMPERATIVE (How to do it)                                │
│  ───────────────────────────                               │
│  1. Create network                                         │
│  2. Create container A                                     │
│  3. Start container A                                      │
│  4. Create container B                                     │
│  5. Start container B                                      │
│  6. Connect containers to network                          │
│                                                            │
│  DECLARATIVE (What you want)                               │
│  ────────────────────────────                              │
│  desired_state:                                            │
│    containers:                                             │
│      - name: app                                           │
│        replicas: 3                                         │
│        image: myapp:v1                                     │
│                                                            │
│  Kubernetes figures out "how" to achieve desired state     │
│                                                            │
└────────────────────────────────────────────────────────────┘
```

### Control Loop (Reconciliation)

```mermaid
graph LR
    A[Desired State<br/>YAML Manifest] --> B[API Server]
    B --> C[etcd<br/>State Store]
    D[Current State<br/>Running Pods] --> E[Controller]
    C --> E
    E --> F{Match?}
    F -->|No| G[Take Action]
    F -->|Yes| H[Do Nothing]
    G --> D
    H --> D
    
    style A fill:#2ecc71
    style D fill:#3498db
    style F fill:#f39c12
    style G fill:#e74c3c
```

---

## Control Plane Components

### API Server

```
┌─────────────────────────────────────────────────────────────┐
│                      API SERVER                             │
│              (Central Management Component)                 │
├─────────────────────────────────────────────────────────────┤
│                                                             │
│  Purpose:                                                   │
│  - Single entry point for all cluster operations           │
│  - RESTful API for cluster management                      │
│  - Authenticates and authorizes requests                   │
│  - Validates and processes API objects                     │
│  - Updates etcd with cluster state                         │
│                                                             │
│  ┌───────────────────────────────────────────────────┐     │
│  │  Request Flow:                                    │     │
│  │                                                   │     │
│  │  kubectl apply -f deployment.yaml                │     │
│  │         │                                         │     │
│  │         ▼                                         │     │
│  │  ┌──────────────┐                                │     │
│  │  │ Authenticate │  (User credentials)            │     │
│  │  └──────┬───────┘                                │     │
│  │         │                                         │     │
│  │         ▼                                         │     │
│  │  ┌──────────────┐                                │     │
│  │  │  Authorize   │  (RBAC policies)               │     │
│  │  └──────┬───────┘                                │     │
│  │         │                                         │     │
│  │         ▼                                         │     │
│  │  ┌──────────────┐                                │     │
│  │  │   Validate   │  (Schema validation)           │     │
│  │  └──────┬───────┘                                │     │
│  │         │                                         │     │
│  │         ▼                                         │     │
│  │  ┌──────────────┐                                │     │
│  │  │  Store etcd  │  (Persist state)               │     │
│  │  └──────┬───────┘                                │     │
│  │         │                                         │     │
│  │         ▼                                         │     │
│  │  ┌──────────────┐                                │     │
│  │  │   Response   │  (Success/Failure)             │     │
│  │  └──────────────┘                                │     │
│  └───────────────────────────────────────────────────┘     │
│                                                             │
│  Port: 443 (HTTPS)                                         │
│  Protocol: RESTful JSON/YAML over HTTP/2                   │
│                                                             │
│  Key Features:                                             │
│  ✓ Horizontal scaling (multiple API servers)              │
│  ✓ TLS encryption                                          │
│  ✓ Admission controllers (webhooks)                        │
│  ✓ API versioning (v1, v1beta1)                           │
│                                                             │
└─────────────────────────────────────────────────────────────┘
```

### etcd

```
┌─────────────────────────────────────────────────────────────┐
│                         etcd                                │
│              (Distributed Key-Value Store)                  │
├─────────────────────────────────────────────────────────────┤
│                                                             │
│  Purpose:                                                   │
│  - Single source of truth for cluster state                │
│  - Stores all cluster configuration and state              │
│  - Consistent and highly-available data store              │
│  - Watches for changes (event-driven)                      │
│                                                             │
│  Data Stored:                                               │
│  ┌───────────────────────────────────────────────────┐     │
│  │  /registry/pods/default/nginx-abc123              │     │
│  │  /registry/services/default/my-service            │     │
│  │  /registry/deployments/default/my-app             │     │
│  │  /registry/secrets/default/db-credentials         │     │
│  │  /registry/configmaps/default/app-config          │     │
│  │  /registry/nodes/worker-node-1                    │     │
│  └───────────────────────────────────────────────────┘     │
│                                                             │
│  Architecture (High Availability):                          │
│  ┌───────────────────────────────────────────────────┐     │
│  │                                                   │     │
│  │   etcd-1  ◄────────► etcd-2  ◄────────► etcd-3   │     │
│  │  (Leader)            (Follower)        (Follower) │     │
│  │                                                   │     │
│  │  Raft Consensus Algorithm                         │     │
│  │  Quorum: (N/2) + 1 nodes must agree              │     │
│  │  Example: 3 nodes = can tolerate 1 failure       │     │
│  │           5 nodes = can tolerate 2 failures      │     │
│  └───────────────────────────────────────────────────┘     │
│                                                             │
│  Port: 2379 (client), 2380 (peer)                          │
│                                                             │
│  Critical Notes:                                           │
│  ⚠️  Backup etcd regularly (cluster state)                 │
│  ⚠️  Use odd number of nodes (3, 5, 7)                     │
│  ⚠️  Secure with TLS/certificates                          │
│  ⚠️  Fast disk required (SSD recommended)                  │
│                                                             │
└─────────────────────────────────────────────────────────────┘
```

### Scheduler

```
┌─────────────────────────────────────────────────────────────┐
│                       SCHEDULER                             │
│              (Pod Placement Decision Maker)                 │
├─────────────────────────────────────────────────────────────┤
│                                                             │
│  Purpose:                                                   │
│  - Assigns pods to nodes                                    │
│  - Considers resource requirements                          │
│  - Evaluates constraints and preferences                    │
│  - Optimizes cluster resource utilization                   │
│                                                             │
│  Scheduling Process:                                        │
│  ┌───────────────────────────────────────────────────┐     │
│  │                                                   │     │
│  │  1. Watch for unscheduled pods                    │     │
│  │     (pod.spec.nodeName == null)                   │     │
│  │                                                   │     │
│  │  2. Filtering Phase (Predicate)                   │     │
│  │     ├─ Sufficient CPU/Memory?                     │     │
│  │     ├─ Node selector match?                       │     │
│  │     ├─ Taints and tolerations?                    │     │
│  │     ├─ Affinity/Anti-affinity rules?              │     │
│  │     └─ Volume availability?                       │     │
│  │                                                   │     │
│  │  3. Scoring Phase (Priority)                      │     │
│  │     ├─ Least requested resources (balance)        │     │
│  │     ├─ Node affinity weight                       │     │
│  │     ├─ Pod topology spread                        │     │
│  │     └─ Image locality                             │     │
│  │                                                   │     │
│  │  4. Bind pod to best node                         │     │
│  │     └─ Update pod.spec.nodeName in etcd          │     │
│  │                                                   │     │
│  └───────────────────────────────────────────────────┘     │
│                                                             │
│  Example Scenario:                                          │
│  ┌───────────────────────────────────────────────────┐     │
│  │  Node A: CPU 50%, Memory 40%  Score: 85          │     │
│  │  Node B: CPU 80%, Memory 70%  Score: 45          │     │
│  │  Node C: CPU 30%, Memory 20%  Score: 95 ✓        │     │
│  │                                                   │     │
│  │  Pod scheduled to Node C (highest score)         │     │
│  └───────────────────────────────────────────────────┘     │
│                                                             │
└─────────────────────────────────────────────────────────────┘
```

### Controller Manager

```
┌─────────────────────────────────────────────────────────────┐
│                  CONTROLLER MANAGER                         │
│              (Reconciliation Loop Manager)                  │
├─────────────────────────────────────────────────────────────┤
│                                                             │
│  Purpose:                                                   │
│  - Runs controller processes                                │
│  - Watches for changes in cluster state                    │
│  - Takes action to match desired state                     │
│  - Ensures cluster health                                   │
│                                                             │
│  Built-in Controllers:                                      │
│                                                             │
│  ┌──────────────────────────────────────────────────┐      │
│  │  Node Controller                                 │      │
│  │  - Monitors node health                          │      │
│  │  - Marks nodes as NotReady after timeout         │      │
│  │  - Evicts pods from failed nodes                 │      │
│  └──────────────────────────────────────────────────┘      │
│                                                             │
│  ┌──────────────────────────────────────────────────┐      │
│  │  Replication Controller                          │      │
│  │  - Maintains correct number of pod replicas      │      │
│  │  - Creates new pods if some fail                 │      │
│  │  - Deletes excess pods                           │      │
│  └──────────────────────────────────────────────────┘      │
│                                                             │
│  ┌──────────────────────────────────────────────────┐      │
│  │  Endpoints Controller                            │      │
│  │  - Populates Endpoints object                    │      │
│  │  - Links Services to Pods                        │      │
│  │  - Updates when pods change                      │      │
│  └──────────────────────────────────────────────────┘      │
│                                                             │
│  ┌──────────────────────────────────────────────────┐      │
│  │  Service Account Controller                      │      │
│  │  - Creates default ServiceAccounts               │      │
│  │  - Manages API tokens                            │      │
│  └──────────────────────────────────────────────────┘      │
│                                                             │
│  Reconciliation Loop:                                       │
│  ┌───────────────────────────────────────────────────┐     │
│  │                                                   │     │
│  │   while (true) {                                  │     │
│  │     desired = read_from_etcd()                    │     │
│  │     current = observe_cluster()                   │     │
│  │                                                   │     │
│  │     if (desired != current) {                     │     │
│  │       take_action_to_fix()                        │     │
│  │     }                                             │     │
│  │                                                   │     │
│  │     sleep(30_seconds)                             │     │
│  │   }                                               │     │
│  │                                                   │     │
│  └───────────────────────────────────────────────────┘     │
│                                                             │
└─────────────────────────────────────────────────────────────┘
```

---

## Worker Node Components

### Kubelet

```
┌─────────────────────────────────────────────────────────────┐
│                        KUBELET                              │
│                  (Node Agent - The Worker)                  │
├─────────────────────────────────────────────────────────────┤
│                                                             │
│  Purpose:                                                   │
│  - Primary node agent running on each worker node          │
│  - Ensures containers are running in pods                  │
│  - Communicates with API server                            │
│  - Reports node and pod status                             │
│  - Manages pod lifecycle                                    │
│                                                             │
│  Responsibilities:                                          │
│  ┌───────────────────────────────────────────────────┐     │
│  │                                                   │     │
│  │  1. Pod Lifecycle Management                      │     │
│  │     ├─ Watch API server for pod assignments      │     │
│  │     ├─ Pull container images                      │     │
│  │     ├─ Start/stop containers                      │     │
│  │     ├─ Mount volumes                              │     │
│  │     └─ Execute liveness/readiness probes         │     │
│  │                                                   │     │
│  │  2. Resource Management                           │     │
│  │     ├─ Enforce resource limits (CPU/Memory)       │     │
│  │     ├─ Report resource usage                      │     │
│  │     └─ Evict pods if node resources low          │     │
│  │                                                   │     │
│  │  3. Health Monitoring                             │     │
│  │     ├─ Run health checks on containers           │     │
│  │     ├─ Restart failed containers                  │     │
│  │     └─ Report pod status to API server           │     │
│  │                                                   │     │
│  │  4. Node Registration                             │     │
│  │     ├─ Register node with API server             │     │
│  │     ├─ Report node capacity                       │     │
│  │     └─ Update node status                         │     │
│  │                                                   │     │
│  └───────────────────────────────────────────────────┘     │
│                                                             │
│  Communication Flow:                                        │
│  ┌───────────────────────────────────────────────────┐     │
│  │                                                   │     │
│  │   API Server                                      │     │
│  │        │                                          │     │
│  │        │ Watch for pod assignments                │     │
│  │        ▼                                          │     │
│  │   Kubelet                                         │     │
│  │        │                                          │     │
│  │        │ Manage containers via CRI                │     │
│  │        ▼                                          │     │
│  │   Container Runtime (containerd/CRI-O)            │     │
│  │        │                                          │     │
│  │        ▼                                          │     │
│  │   Containers in Pods                              │     │
│  │                                                   │     │
│  └───────────────────────────────────────────────────┘     │
│                                                             │
│  Port: 10250 (API), 10255 (Read-only)                     │
│                                                             │
└─────────────────────────────────────────────────────────────┘
```

### Kube-Proxy

```
┌─────────────────────────────────────────────────────────────┐
│                      KUBE-PROXY                             │
│              (Network Proxy - Service Router)               │
├─────────────────────────────────────────────────────────────┤
│                                                             │
│  Purpose:                                                   │
│  - Maintains network rules on nodes                        │
│  - Implements Kubernetes Service abstraction               │
│  - Load balances traffic to pods                           │
│  - Handles service discovery and routing                   │
│                                                             │
│  How Services Work:                                         │
│  ┌───────────────────────────────────────────────────┐     │
│  │                                                   │     │
│  │  Service: my-app (ClusterIP: 10.96.0.10)         │     │
│  │     │                                             │     │
│  │     │ kube-proxy creates iptables rules          │     │
│  │     ▼                                             │     │
│  │  ┌──────────────────────────────────────┐        │     │
│  │  │  iptables / IPVS rules:              │        │     │
│  │  │                                      │        │     │
│  │  │  10.96.0.10:80 ──► Pod1: 10.244.1.5 │        │     │
│  │  │                 ──► Pod2: 10.244.2.7 │        │     │
│  │  │                 ──► Pod3: 10.244.3.9 │        │     │
│  │  │                                      │        │     │
│  │  │  Round-robin load balancing          │        │     │
│  │  └──────────────────────────────────────┘        │     │
│  │                                                   │     │
│  └───────────────────────────────────────────────────┘     │
│                                                             │
│  Modes of Operation:                                        │
│                                                             │
│  1. iptables Mode (default)                                │
│     - Uses iptables rules for routing                      │
│     - Random selection for load balancing                  │
│     - No real load balancing (just distribution)           │
│                                                             │
│  2. IPVS Mode (recommended for large clusters)             │
│     - Uses Linux IPVS for load balancing                   │
│     - Better performance at scale                          │
│     - Multiple load balancing algorithms:                  │
│       * Round-robin                                        │
│       * Least connection                                   │
│       * Source hashing                                     │
│                                                             │
│  3. Userspace Mode (legacy)                                │
│     - kube-proxy as actual proxy                           │
│     - Rarely used in modern deployments                    │
│                                                             │
│  Network Flow Example:                                      │
│  ┌───────────────────────────────────────────────────┐     │
│  │                                                   │     │
│  │  Client (10.244.1.2)                              │     │
│  │     │                                             │     │
│  │     │ Request to Service IP: 10.96.0.10:80       │     │
│  │     ▼                                             │     │
│  │  iptables NAT                                     │     │
│  │     │                                             │     │
│  │     │ Translated to: 10.244.2.7:8080             │     │
│  │     ▼                                             │     │
│  │  Backend Pod                                      │     │
│  │                                                   │     │
│  └───────────────────────────────────────────────────┘     │
│                                                             │
└─────────────────────────────────────────────────────────────┘
```

### Container Runtime

```
┌─────────────────────────────────────────────────────────────┐
│                   CONTAINER RUNTIME                         │
│              (Container Execution Engine)                   │
├─────────────────────────────────────────────────────────────┤
│                                                             │
│  Purpose:                                                   │
│  - Responsible for running containers                      │
│  - Pulls container images from registry                    │
│  - Manages container lifecycle                             │
│  - Provides container isolation                            │
│                                                             │
│  Evolution:                                                 │
│  ┌───────────────────────────────────────────────────┐     │
│  │                                                   │     │
│  │  Kubernetes 1.0-1.23 (Docker as default)          │     │
│  │  ┌──────────────────────────────────────┐        │     │
│  │  │  Kubelet → dockershim → Docker        │        │     │
│  │  └──────────────────────────────────────┘        │     │
│  │                                                   │     │
│  │  Kubernetes 1.24+ (CRI-compatible runtimes)      │     │
│  │  ┌──────────────────────────────────────┐        │     │
│  │  │  Kubelet → CRI → containerd/CRI-O     │        │     │
│  │  └──────────────────────────────────────┘        │     │
│  │                                                   │     │
│  └───────────────────────────────────────────────────┘     │
│                                                             │
│  Supported Container Runtimes:                              │
│                                                             │
│  1. containerd (Most Popular)                               │
│     - Industry standard                                     │
│     - CNCF graduated project                                │
│     - Docker's underlying runtime                           │
│     - Lightweight and efficient                             │
│                                                             │
│  2. CRI-O (Red Hat)                                         │
│     - Built specifically for Kubernetes                     │
│     - Lightweight OCI-compliant runtime                     │
│     - Minimal dependencies                                  │
│                                                             │
│  3. Docker Engine (via cri-dockerd)                         │
│     - Requires additional shim layer                        │
│     - Legacy support                                        │
│     - Not recommended for new clusters                      │
│                                                             │
│  Container Runtime Interface (CRI):                         │
│  ┌───────────────────────────────────────────────────┐     │
│  │                                                   │     │
│  │   Kubelet (gRPC Client)                           │     │
│  │        │                                          │     │
│  │        │ CRI API calls                            │     │
│  │        ▼                                          │     │
│  │   CRI Plugin (gRPC Server)                        │     │
│  │        │                                          │     │
│  │        │ Runtime Operations                       │     │
│  │        ▼                                          │     │
│  │   Low-level Runtime (runc/kata)                   │     │
│  │                                                   │     │
│  └───────────────────────────────────────────────────┘     │
│                                                             │
└─────────────────────────────────────────────────────────────┘
```

---

## Core Objects

### Pod - Smallest Deployable Unit

```
┌─────────────────────────────────────────────────────────────┐
│                           POD                               │
│              (Atomic Unit of Deployment)                    │
├─────────────────────────────────────────────────────────────┤
│                                                             │
│  Definition:                                                │
│  - Smallest deployable unit in Kubernetes                  │
│  - One or more containers sharing resources                │
│  - Ephemeral by nature (not durable)                       │
│  - Each pod has unique IP address                          │
│  - Containers in pod share network namespace               │
│                                                             │
│  Pod Structure:                                             │
│  ┌───────────────────────────────────────────────────┐     │
│  │                 Pod: nginx-abc123                 │     │
│  │                 IP: 10.244.1.5                    │     │
│  │                                                   │     │
│  │  ┌─────────────────────────────────────────────┐ │     │
│  │  │  Pause Container (Infrastructure)           │ │     │
│  │  │  - Holds network namespace                  │ │     │
│  │  │  - Minimal resource footprint               │ │     │
│  │  └─────────────────────────────────────────────┘ │     │
│  │                                                   │     │
│  │  ┌─────────────────────┐  ┌──────────────────┐  │     │
│  │  │  Main Container     │  │  Sidecar         │  │     │
│  │  │  nginx:1.21         │  │  log-collector   │  │     │
│  │  │  Port: 80           │  │  Port: 9090      │  │     │
│  │  └─────────────────────┘  └──────────────────┘  │     │
│  │                                                   │     │
│  │  Shared Resources:                                │     │
│  │  ├─ Network (localhost communication)            │     │
│  │  ├─ IPC (Inter-Process Communication)            │     │
│  │  ├─ Volumes (shared storage)                     │     │
│  │  └─ PID namespace (optional)                     │     │
│  │                                                   │     │
│  └───────────────────────────────────────────────────┘     │
│                                                             │
│  Pod Lifecycle States:                                      │
│  ┌───────────────────────────────────────────────────┐     │
│  │                                                   │     │
│  │  Pending    → Container images being pulled      │     │
│  │  Running    → At least one container running     │     │
│  │  Succeeded  → All containers terminated success  │     │
│  │  Failed     → At least one container failed      │     │
│  │  Unknown    → Cannot determine pod state         │     │
│  │                                                   │     │
│  └───────────────────────────────────────────────────┘     │
│                                                             │
│  YAML Example:                                              │
│  ```yaml                                                    │
│  apiVersion: v1                                             │
│  kind: Pod                                                  │
│  metadata:                                                  │
│    name: nginx-pod                                          │
│    labels:                                                  │
│      app: nginx                                             │
│  spec:                                                      │
│    containers:                                              │
│    - name: nginx                                            │
│      image: nginx:1.21                                      │
│      ports:                                                 │
│      - containerPort: 80                                    │
│  ```                                                        │
│                                                             │
└─────────────────────────────────────────────────────────────┘
```

### ReplicaSet - Desired State Maintenance

```
┌─────────────────────────────────────────────────────────────┐
│                       REPLICASET                            │
│            (Maintains Desired Number of Pods)               │
├─────────────────────────────────────────────────────────────┤
│                                                             │
│  Purpose:                                                   │
│  - Ensures specified number of pod replicas running        │
│  - Self-healing: Creates pods if they fail                 │
│  - Horizontal scaling capability                           │
│  - Uses label selectors to identify pods                   │
│                                                             │
│  How It Works:                                              │
│  ┌───────────────────────────────────────────────────┐     │
│  │                                                   │     │
│  │  Desired State: replicas = 3                      │     │
│  │                                                   │     │
│  │  ┌─────────┐  ┌─────────┐  ┌─────────┐          │     │
│  │  │  Pod 1  │  │  Pod 2  │  │  Pod 3  │          │     │
│  │  │ Running │  │ Running │  │ Running │          │     │
│  │  └─────────┘  └─────────┘  └─────────┘          │     │
│  │                                                   │     │
│  │  Current State: 3 pods running  ✓                │     │
│  │                                                   │     │
│  └───────────────────────────────────────────────────┘     │
│                                                             │
│  Failure Scenario:                                          │
│  ┌───────────────────────────────────────────────────┐     │
│  │                                                   │     │
│  │  Desired State: replicas = 3                      │     │
│  │                                                   │     │
│  │  ┌─────────┐  ┌─────────┐  ┌─────────┐          │     │
│  │  │  Pod 1  │  │  Pod 2  │  │  Pod 3  │          │     │
│  │  │ Running │  │ Failed  │  │ Running │          │     │
│  │  └─────────┘  └────X────┘  └─────────┘          │     │
│  │                     │                             │     │
│  │                     │ ReplicaSet detects failure │     │
│  │                     ▼                             │     │
│  │                ┌─────────┐                        │     │
│  │                │  Pod 4  │                        │     │
│  │                │ Creating│                        │     │
│  │                └─────────┘                        │     │
│  │                                                   │     │
│  │  Current State: Auto-healing in progress         │     │
│  │                                                   │     │
│  └───────────────────────────────────────────────────┘     │
│                                                             │
│  YAML Example:                                              │
│  ```yaml                                                    │
│  apiVersion: apps/v1                                        │
│  kind: ReplicaSet                                           │
│  metadata:                                                  │
│    name: nginx-rs                                           │
│  spec:                                                      │
│    replicas: 3                                              │
│    selector:                                                │
│      matchLabels:                                           │
│        app: nginx                                           │
│    template:                                                │
│      metadata:                                              │
│        labels:                                              │
│          app: nginx                                         │
│      spec:                                                  │
│        containers:                                          │
│        - name: nginx                                        │
│          image: nginx:1.21                                  │
│  ```                                                        │
│                                                             │
└─────────────────────────────────────────────────────────────┘
```

### Deployment - Declarative Updates

```
┌─────────────────────────────────────────────────────────────┐
│                       DEPLOYMENT                            │
│         (Manages ReplicaSets and Rolling Updates)           │
├─────────────────────────────────────────────────────────────┤
│                                                             │
│  Purpose:                                                   │
│  - Declarative updates for pods and ReplicaSets            │
│  - Rolling updates and rollbacks                           │
│  - Manages multiple ReplicaSet versions                    │
│  - Deployment strategies (RollingUpdate, Recreate)         │
│                                                             │
│  Hierarchy:                                                 │
│  ┌───────────────────────────────────────────────────┐     │
│  │                                                   │     │
│  │              Deployment                           │     │
│  │                  │                                │     │
│  │         ┌────────┴────────┐                      │     │
│  │         ▼                 ▼                      │     │
│  │   ReplicaSet v1      ReplicaSet v2               │     │
│  │   (replicas: 0)      (replicas: 3)               │     │
│  │                           │                       │     │
│  │                   ┌───────┼───────┐              │     │
│  │                   ▼       ▼       ▼              │     │
│  │                 Pod1    Pod2    Pod3             │     │
│  │                                                   │     │
│  └───────────────────────────────────────────────────┘     │
│                                                             │
│  Rolling Update Process:                                    │
│  ┌───────────────────────────────────────────────────┐     │
│  │                                                   │     │
│  │  Step 1: Old ReplicaSet (v1) - 3 pods            │     │
│  │  ┌─────┐  ┌─────┐  ┌─────┐                      │     │
│  │  │ v1  │  │ v1  │  │ v1  │                      │     │
│  │  └─────┘  └─────┘  └─────┘                      │     │
│  │                                                   │     │
│  │  Step 2: New ReplicaSet (v2) created             │     │
│  │  ┌─────┐  ┌─────┐  ┌─────┐  ┌─────┐             │     │
│  │  │ v1  │  │ v1  │  │ v1  │  │ v2  │             │     │
│  │  └─────┘  └─────┘  └─────┘  └─────┘             │     │
│  │                                                   │     │
│  │  Step 3: Gradually replace pods                  │     │
│  │  ┌─────┐  ┌─────┐  ┌─────┐  ┌─────┐             │     │
│  │  │ v1  │  │ v2  │  │ v2  │  │ v2  │             │     │
│  │  └─────┘  └─────┘  └─────┘  └─────┘             │     │
│  │                                                   │     │
│  │  Step 4: All pods updated to v2                  │     │
│  │  ┌─────┐  ┌─────┐  ┌─────┐                      │     │
│  │  │ v2  │  │ v2  │  │ v2  │                      │     │
│  │  └─────┘  └─────┘  └─────┘                      │     │
│  │                                                   │     │
│  │  Zero downtime during update!                    │     │
│  │                                                   │     │
│  └───────────────────────────────────────────────────┘     │
│                                                             │
│  Deployment Strategies:                                     │
│                                                             │
│  1. RollingUpdate (Default)                                 │
│     - Gradually replace old pods with new                  │
│     - maxSurge: Max new pods during update                 │
│     - maxUnavailable: Max unavailable pods                 │
│     - Zero downtime                                        │
│                                                             │
│  2. Recreate                                                │
│     - Kill all old pods first                              │
│     - Then create new pods                                 │
│     - Downtime during update                               │
│     - Simpler, faster                                      │
│                                                             │
│  YAML Example:                                              │
│  ```yaml                                                    │
│  apiVersion: apps/v1                                        │
│  kind: Deployment                                           │
│  metadata:                                                  │
│    name: nginx-deployment                                   │
│  spec:                                                      │
│    replicas: 3                                              │
│    strategy:                                                │
│      type: RollingUpdate                                    │
│      rollingUpdate:                                         │
│        maxSurge: 1                                          │
│        maxUnavailable: 1                                    │
│    selector:                                                │
│      matchLabels:                                           │
│        app: nginx                                           │
│    template:                                                │
│      metadata:                                              │
│        labels:                                              │
│          app: nginx                                         │
│      spec:                                                  │
│        containers:                                          │
│        - name: nginx                                        │
│          image: nginx:1.21                                  │
│          ports:                                             │
│          - containerPort: 80                                │
│  ```                                                        │
│                                                             │
└─────────────────────────────────────────────────────────────┘
```

### Service - Stable Network Endpoint

```
┌─────────────────────────────────────────────────────────────┐
│                        SERVICE                              │
│              (Stable Network Abstraction)                   │
├─────────────────────────────────────────────────────────────┤
│                                                             │
│  Purpose:                                                   │
│  - Stable IP and DNS name for pod group                    │
│  - Load balancing across pod replicas                      │
│  - Service discovery                                        │
│  - Decouples frontend from backend pods                    │
│                                                             │
│  Problem Without Service:                                   │
│  ┌───────────────────────────────────────────────────┐     │
│  │                                                   │     │
│  │  Frontend needs to connect to backend            │     │
│  │                                                   │     │
│  │  Backend Pods (IPs change on restart):           │     │
│  │  - Pod 1: 10.244.1.5 (killed)                    │     │
│  │  - Pod 2: 10.244.2.7 (killed)                    │     │
│  │  - Pod 3: 10.244.1.15 (new IP!)                  │     │
│  │  - Pod 4: 10.244.2.18 (new IP!)                  │     │
│  │                                                   │     │
│  │  How does frontend find backends? ❌              │     │
│  │                                                   │     │
│  └───────────────────────────────────────────────────┘     │
│                                                             │
│  Solution With Service:                                     │
│  ┌───────────────────────────────────────────────────┐     │
│  │                                                   │     │
│  │  Service: backend-service                         │     │
│  │  ClusterIP: 10.96.0.10 (stable)                  │     │
│  │  DNS: backend-service.default.svc.cluster.local  │     │
│  │                                                   │     │
│  │          │ Selector: app=backend                  │     │
│  │          ▼                                        │     │
│  │  ┌─────────────────────────────┐                 │     │
│  │  │  Backend Pods:              │                 │     │
│  │  │  - Pod A: 10.244.1.15       │                 │     │
│  │  │  - Pod B: 10.244.2.18       │                 │     │
│  │  │  - Pod C: 10.244.3.22       │                 │     │
│  │  └─────────────────────────────┘                 │     │
│  │                                                   │     │
│  │  Frontend connects to: backend-service:8080 ✓    │     │
│  │  Service load balances to healthy pods           │     │
│  │                                                   │     │
│  └───────────────────────────────────────────────────┘     │
│                                                             │
│  Service Types:                                             │
│                                                             │
│  1. ClusterIP (Default)                                     │
│     - Internal cluster IP                                  │
│     - Only accessible within cluster                       │
│     - Use for internal services                            │
│                                                             │
│  2. NodePort                                                │
│     - Exposes service on each node's IP                    │
│     - Accessible from outside cluster                      │
│     - Port range: 30000-32767                              │
│     - Format: <NodeIP>:<NodePort>                          │
│                                                             │
│  3. LoadBalancer                                            │
│     - Creates external load balancer (cloud provider)      │
│     - Gets external IP address                             │
│     - Routes traffic to NodePort                           │
│                                                             │
│  4. ExternalName                                            │
│     - Maps service to external DNS name                    │
│     - CNAME record                                         │
│     - No proxy or load balancing                           │
│                                                             │
│  YAML Example:                                              │
│  ```yaml                                                    │
│  apiVersion: v1                                             │
│  kind: Service                                              │
│  metadata:                                                  │
│    name: backend-service                                    │
│  spec:                                                      │
│    type: ClusterIP                                          │
│    selector:                                                │
│      app: backend                                           │
│    ports:                                                   │
│    - protocol: TCP                                          │
│      port: 8080         # Service port                     │
│      targetPort: 8080   # Container port                   │
│  ```                                                        │
│                                                             │
└─────────────────────────────────────────────────────────────┘
```

### ConfigMap and Secret

```
┌─────────────────────────────────────────────────────────────┐
│                 CONFIGMAP & SECRET                          │
│            (Configuration and Sensitive Data)               │
├─────────────────────────────────────────────────────────────┤
│                                                             │
│  ConfigMap - Non-sensitive Configuration                    │
│  ┌───────────────────────────────────────────────────┐     │
│  │                                                   │     │
│  │  Purpose:                                         │     │
│  │  - Store configuration data as key-value pairs    │     │
│  │  - Decouple configuration from container images  │     │
│  │  - Allow dynamic configuration updates           │     │
│  │                                                   │     │
│  │  Usage Methods:                                   │     │
│  │  1. Environment variables                         │     │
│  │  2. Command-line arguments                        │     │
│  │  3. Configuration files in volume                 │     │
│  │                                                   │     │
│  │  Example:                                         │     │
│  │  ```yaml                                          │     │
│  │  apiVersion: v1                                   │     │
│  │  kind: ConfigMap                                  │     │
│  │  metadata:                                        │     │
│  │    name: app-config                               │     │
│  │  data:                                            │     │
│  │    APP_MODE: "production"                         │     │
│  │    LOG_LEVEL: "info"                              │     │
│  │    DATABASE_HOST: "postgres.default.svc"          │     │
│  │    app.properties: |                              │     │
│  │      max.connections=100                          │     │
│  │      timeout=30                                   │     │
│  │  ```                                              │     │
│  │                                                   │     │
│  └───────────────────────────────────────────────────┘     │
│                                                             │
│  Secret - Sensitive Configuration                           │
│  ┌───────────────────────────────────────────────────┐     │
│  │                                                   │     │
│  │  Purpose:                                         │     │
│  │  - Store sensitive data (passwords, tokens, keys) │     │
│  │  - Base64 encoded (not encrypted by default!)    │     │
│  │  - More secure than ConfigMaps                    │     │
│  │  - Can be encrypted at rest (etcd encryption)    │     │
│  │                                                   │     │
│  │  Types:                                           │     │
│  │  - Opaque (default, arbitrary data)              │     │
│  │  - kubernetes.io/service-account-token            │     │
│  │  - kubernetes.io/dockerconfigjson                 │     │
│  │  - kubernetes.io/tls                              │     │
│  │                                                   │     │
│  │  Example:                                         │     │
│  │  ```yaml                                          │     │
│  │  apiVersion: v1                                   │     │
│  │  kind: Secret                                     │     │
│  │  metadata:                                        │     │
│  │    name: db-secret                                │     │
│  │  type: Opaque                                     │     │
│  │  data:                                            │     │
│  │    username: YWRtaW4=      # base64: admin       │     │
│  │    password: czNjcjN0cGFzcw==  # base64: s3cr3tpass │
│  │  ```                                              │     │
│  │                                                   │     │
│  └───────────────────────────────────────────────────┘     │
│                                                             │
│  Using ConfigMap/Secret in Pod:                             │
│  ```yaml                                                    │
│  apiVersion: v1                                             │
│  kind: Pod                                                  │
│  metadata:                                                  │
│    name: myapp                                              │
│  spec:                                                      │
│    containers:                                              │
│    - name: app                                              │
│      image: myapp:1.0                                       │
│      env:                                                   │
│      # From ConfigMap                                       │
│      - name: APP_MODE                                       │
│        valueFrom:                                           │
│          configMapKeyRef:                                   │
│            name: app-config                                 │
│            key: APP_MODE                                    │
│      # From Secret                                          │
│      - name: DB_PASSWORD                                    │
│        valueFrom:                                           │
│          secretKeyRef:                                      │
│            name: db-secret                                  │
│            key: password                                    │
│      # Mount as volume                                      │
│      volumeMounts:                                          │
│      - name: config-volume                                  │
│        mountPath: /etc/config                               │
│    volumes:                                                 │
│    - name: config-volume                                    │
│      configMap:                                             │
│        name: app-config                                     │
│  ```                                                        │
│                                                             │
└─────────────────────────────────────────────────────────────┘
```

---

## Networking Architecture

### Kubernetes Networking Model

```ascii
┌────────────────────────────────────────────────────────────┐
│           KUBERNETES NETWORKING PRINCIPLES                 │
├────────────────────────────────────────────────────────────┤
│                                                            │
│  Fundamental Rules:                                        │
│                                                            │
│  1. Every pod gets its own IP address                     │
│     - No NAT between pods                                  │
│     - Pods can communicate with all other pods            │
│                                                            │
│  2. All nodes can communicate with all pods               │
│     - Without NAT                                          │
│     - In both directions                                   │
│                                                            │
│  3. Pod sees itself with same IP others see it            │
│     - No network translation                               │
│     - Simplified networking model                          │
│                                                            │
└────────────────────────────────────────────────────────────┘
```

### Network Topology

```
┌──────────────────────────────────────────────────────────────┐
│                 KUBERNETES CLUSTER NETWORK                   │
├──────────────────────────────────────────────────────────────┤
│                                                              │
│  ┌────────────────────────────────────────────────────────┐ │
│  │              Cluster Network (10.96.0.0/12)            │ │
│  │                   (Service IPs)                        │ │
│  │                                                        │ │
│  │  Service A: 10.96.0.10                                 │ │
│  │  Service B: 10.96.0.20                                 │ │
│  │  Service C: 10.96.0.30                                 │ │
│  └────────────────────────────────────────────────────────┘ │
│                             │                                │
│                             │ kube-proxy                     │
│                             ▼                                │
│  ┌────────────────────────────────────────────────────────┐ │
│  │              Pod Network (10.244.0.0/16)               │ │
│  │                                                        │ │
│  │  ┌──────────────────┐      ┌──────────────────┐      │ │
│  │  │   Node 1         │      │   Node 2         │      │ │
│  │  │   10.244.1.0/24  │      │   10.244.2.0/24  │      │ │
│  │  │                  │      │                  │      │ │
│  │  │  ┌────┐  ┌────┐  │      │  ┌────┐  ┌────┐  │      │ │
│  │  │  │Pod │  │Pod │  │      │  │Pod │  │Pod │  │      │ │
│  │  │  │.1.5│  │.1.6│  │      │  │.2.3│  │.2.4│  │      │ │
│  │  │  └────┘  └────┘  │      │  └────┘  └────┘  │      │ │
│  │  └──────────────────┘      └──────────────────┘      │ │
│  │                                                        │ │
│  │  CNI Plugin (Calico/Flannel/Weave) manages routing    │ │
│  └────────────────────────────────────────────────────────┘ │
│                             │                                │
│                             │                                │
│                             ▼                                │
│  ┌────────────────────────────────────────────────────────┐ │
│  │          External Network (Internet)                   │ │
│  │                                                        │ │
│  │  Ingress Controllers expose services externally       │ │
│  └────────────────────────────────────────────────────────┘ │
│                                                              │
└──────────────────────────────────────────────────────────────┘
```

### Service Types Communication

```mermaid
graph TB
    subgraph "External World"
        A[User Browser]
    end
    
    subgraph "Cluster"
        B[LoadBalancer Service<br/>External IP: 203.0.113.10]
        C[NodePort Service<br/>Port: 30080]
        D[ClusterIP Service<br/>IP: 10.96.0.10]
        E[Pod 1<br/>10.244.1.5]
        F[Pod 2<br/>10.244.2.7]
        G[Pod 3<br/>10.244.3.9]
    end
    
    A -->|Internet| B
    B --> C
    C --> D
    D --> E
    D --> F
    D --> G
    
    style A fill:#3498db
    style B fill:#e74c3c
    style C fill:#f39c12
    style D fill:#2ecc71
```

### DNS in Kubernetes

```
┌─────────────────────────────────────────────────────────────┐
│                   KUBERNETES DNS                            │
│                    (CoreDNS)                                │
├─────────────────────────────────────────────────────────────┤
│                                                             │
│  Service Discovery via DNS:                                 │
│                                                             │
│  Service: my-service (Namespace: default)                   │
│  ┌───────────────────────────────────────────────────┐     │
│  │                                                   │     │
│  │  Full FQDN:                                       │     │
│  │  my-service.default.svc.cluster.local             │     │
│  │  │          │       │   │                         │     │
│  │  │          │       │   └─ Cluster domain         │     │
│  │  │          │       └───── "svc" (service)        │     │
│  │  │          └─────────────  Namespace             │     │
│  │  └────────────────────────── Service name         │     │
│  │                                                   │     │
│  └───────────────────────────────────────────────────┘     │
│                                                             │
│  DNS Resolution Hierarchy:                                  │
│  ┌───────────────────────────────────────────────────┐     │
│  │                                                   │     │
│  │  Same Namespace:                                  │     │
│  │  curl http://my-service:8080                      │     │
│  │                                                   │     │
│  │  Different Namespace:                             │     │
│  │  curl http://my-service.production:8080           │     │
│  │                                                   │     │
│  │  Full FQDN (any namespace):                       │     │
│  │  curl http://my-service.production.svc.cluster.local │  │
│  │                                                   │     │
│  └───────────────────────────────────────────────────┘     │
│                                                             │
│  CoreDNS Deployment:                                        │
│  ┌───────────────────────────────────────────────────┐     │
│  │                                                   │     │
│  │  Pod queries DNS                                  │     │
│  │        │                                          │     │
│  │        ▼                                          │     │
│  │  CoreDNS Pod (running in kube-system namespace)   │     │
│  │        │                                          │     │
│  │        ├─ Checks local cache                      │     │
│  │        ├─ Queries Kubernetes API for Services     │     │
│  │        ├─ Returns ClusterIP                       │     │
│  │        └─ Forwards external queries to upstream   │     │
│  │                                                   │     │
│  └───────────────────────────────────────────────────┘     │
│                                                             │
└─────────────────────────────────────────────────────────────┘
```

---

## Storage Architecture

### Volume Types

```markmap
# Kubernetes Storage
## Ephemeral Volumes
### emptyDir
- Temporary storage
- Shared between containers
- Deleted with pod
### configMap / secret
- Configuration data
- Mounted as files
- Read-only
## Persistent Volumes
### hostPath
- Node local storage
- Testing only
- Not portable
### NFS
- Network file system
- Multi-node access
- Legacy systems
### Cloud Volumes
- AWS EBS
- Azure Disk
- GCP Persistent Disk
- Managed by provider
### CSI Drivers
- Container Storage Interface
- Vendor plugins
- Modern standard
```

### Persistent Storage Flow

```mermaid
sequenceDiagram
    participant Pod
    participant PVC as PersistentVolumeClaim
    participant PV as PersistentVolume
    participant Storage as Physical Storage

    Note over Pod,Storage: Static Provisioning
    Storage->>PV: Admin creates PV
    Pod->>PVC: Pod requests storage via PVC
    PVC->>PV: Binds to matching PV
    PV->>Storage: Mounts physical storage
    Storage->>Pod: Volume available
    
    Note over Pod,Storage: Dynamic Provisioning
    Pod->>PVC: Pod requests storage
    PVC->>Storage: StorageClass auto-provisions
    Storage->>PV: Creates PV automatically
    PV->>Pod: Volume ready
```

---

## Interview Preparation

### Critical Concepts Checklist

```ascii
╔════════════════════════════════════════════════════════════╗
║         KUBERNETES INTERVIEW CHECKLIST                     ║
╠════════════════════════════════════════════════════════════╣
║                                                            ║
║  [ ] Control plane vs worker node components              ║
║  [ ] Pod lifecycle and states                             ║
║  [ ] Deployment vs ReplicaSet vs StatefulSet              ║
║  [ ] Service types and use cases                          ║
║  [ ] ConfigMap vs Secret                                  ║
║  [ ] Ingress vs Service vs LoadBalancer                   ║
║  [ ] Persistent volumes and claims                        ║
║  [ ] Namespaces and resource quotas                       ║
║  [ ] Labels and selectors                                 ║
║  [ ] Health checks (liveness, readiness, startup)         ║
║  [ ] Rolling updates and rollback strategies              ║
║  [ ] Horizontal Pod Autoscaler (HPA)                      ║
║  [ ] RBAC (Role-Based Access Control)                     ║
║  [ ] Network policies                                     ║
║  [ ] Helm charts and package management                   ║
║                                                            ║
╚════════════════════════════════════════════════════════════╝
```

### One-Liner Answers

```ascii
┌────────────────────────────────────────────────────────────┐
│              KUBERNETES INTERVIEW ONE-LINERS               │
├────────────────────────────────────────────────────────────┤
│                                                            │
│  Q: What is Kubernetes?                                    │
│  A: Open-source container orchestration platform that     │
│     automates deployment, scaling, and management of      │
│     containerized applications.                            │
│                                                            │
│  Q: What is a Pod?                                         │
│  A: Smallest deployable unit in Kubernetes, consisting    │
│     of one or more containers sharing network and storage. │
│                                                            │
│  Q: Difference between Deployment and StatefulSet?        │
│  A: Deployment for stateless apps with no persistent      │
│     identity; StatefulSet for stateful apps requiring     │
│     stable network identity and persistent storage.       │
│                                                            │
│  Q: What does kubectl do?                                  │
│  A: Command-line tool to interact with Kubernetes API     │
│     server for managing cluster resources.                │
│                                                            │
│  Q: How does Service discovery work?                       │
│  A: CoreDNS provides DNS-based service discovery using    │
│     format: <service>.<namespace>.svc.cluster.local       │
│                                                            │
│  Q: What is etcd?                                          │
│  A: Distributed key-value store holding all cluster       │
│     state and configuration data.                          │
│                                                            │
│  Q: Explain rolling update strategy                        │
│  A: Gradually replaces old pods with new ones, maintaining│
│     service availability with zero downtime.               │
│                                                            │
│  Q: What is Ingress?                                       │
│  A: API object managing external HTTP/HTTPS access to     │
│     services, providing load balancing, SSL termination,  │
│     and name-based virtual hosting.                        │
│                                                            │
└────────────────────────────────────────────────────────────┘
```

### Common Interview Questions with Detailed Answers

#### Q1: Explain Kubernetes architecture and its main components

**Answer:** Kubernetes architecture consists of two main parts:

**Control Plane (Master Node):**

- **API Server**: Central management point, exposes Kubernetes API
- **etcd**: Distributed key-value store for cluster state
- **Scheduler**: Assigns pods to nodes based on resources
- **Controller Manager**: Runs controllers to maintain desired state

**Worker Nodes:**

- **Kubelet**: Ensures containers are running in pods
- **Kube-proxy**: Maintains network rules for services
- **Container Runtime**: Runs containers (containerd, CRI-O)

Control plane makes decisions, worker nodes execute workloads.

#### Q2: What is the difference between Monolithic and Microservices architecture?

**Answer:** **Monolithic:**

- Single unified codebase
- All components tightly coupled
- Single deployment unit
- Scale entire application
- Technology lock-in

**Microservices:**

- Multiple independent services
- Loosely coupled via APIs
- Independent deployment per service
- Scale individual services
- Technology flexibility

Kubernetes is designed for microservices architecture, providing orchestration, service discovery, and scaling capabilities.

#### Q3: How does Kubernetes achieve self-healing?

**Answer:** Self-healing through controller reconciliation loops:

1. **Liveness Probes**: Restart unhealthy containers
2. **Readiness Probes**: Remove unhealthy pods from service endpoints
3. **ReplicaSet Controller**: Maintains desired pod count
4. **Node Controller**: Detects node failures and reschedules pods
5. **Deployment Controller**: Manages rolling updates

Example: If pod crashes, ReplicaSet detects mismatch between desired (3) and current (2) state, automatically creates replacement pod.

#### Q4: Explain the declarative model in Kubernetes

**Answer:** Declarative model means you specify **what** you want (desired state), not **how** to achieve it.

**Imperative (Docker):**

```bash
docker network create app_net
docker run -d --name db --network app_net mysql
docker run -d --name app --network app_net myapp
```

**Declarative (Kubernetes):**

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: myapp
spec:
  containers:
  - name: app
    image: myapp:1.0
```

Kubernetes continuously reconciles current state with desired state through control loops. If reality deviates, controllers take corrective action automatically.

#### Q5: What is the difference between ConfigMap and Secret?

**Answer:** Both store configuration data, but with different purposes:

**ConfigMap:**

- Non-sensitive configuration
- Plaintext storage
- Database URLs, feature flags, config files
- No encryption

**Secret:**

- Sensitive data (passwords, tokens, keys)
- Base64 encoded (not encrypted by default!)
- Can be encrypted at rest with etcd encryption
- More restricted access via RBAC

**Best Practice:** Enable etcd encryption for production secrets, use external secret management (Vault, AWS Secrets Manager) for enhanced security.

#### Q6: How does service discovery work in Kubernetes?

**Answer:** Kubernetes uses DNS-based service discovery via CoreDNS:

1. **Service Created**: ClusterIP assigned (e.g., 10.96.0.10)
2. **DNS Record Created**: `my-service.default.svc.cluster.local`
3. **Pod Lookup**: Application queries service name
4. **CoreDNS Resolution**: Returns ClusterIP
5. **kube-proxy Routing**: Load balances to healthy pod IPs

**Within Same Namespace:**

```bash
curl http://backend-service:8080
```

**Different Namespace:**

```bash
curl http://backend-service.production:8080
```

No hardcoded IPs needed - services provide stable endpoints.

#### Q7: What is the role of kube-proxy?

**Answer:** kube-proxy is a network proxy running on each node that implements Kubernetes Service abstraction:

**Functions:**

- Maintains network rules (iptables/IPVS)
- Performs connection forwarding
- Load balances traffic across pod replicas
- Enables service-to-pod communication

**Modes:**

- **iptables** (default): Uses iptables rules for routing
- **IPVS**: Better performance for large clusters
- **userspace**: Legacy, rarely used

When you access Service IP (10.96.0.10:80), kube-proxy routes to one of the backend pods (10.244.1.5:8080) using configured load balancing algorithm.

#### Q8: Explain Persistent Volumes vs Persistent Volume Claims

**Answer:** **Persistent Volume (PV):**

- Cluster-level resource
- Admin-provisioned or dynamically created
- Represents actual storage (AWS EBS, NFS, etc.)
- Lifecycle independent of pods

**Persistent Volume Claim (PVC):**

- Namespace-scoped resource
- User's request for storage
- Binds to matching PV
- Used by pods to mount storage

**Analogy:**

- PV = Physical hard drive
- PVC = Purchase order for storage
- Pod = Application using the storage

**Dynamic Provisioning:** StorageClass automatically creates PV when PVC requested, eliminating manual PV creation.

### Quick Command Reference

```bash
# Cluster Information
kubectl cluster-info
kubectl get nodes
kubectl get componentstatuses

# Resource Management
kubectl get pods
kubectl get pods -o wide
kubectl describe pod <pod-name>
kubectl logs <pod-name>
kubectl exec -it <pod-name> -- /bin/bash

# Deployments
kubectl create deployment nginx --image=nginx
kubectl get deployments
kubectl scale deployment nginx --replicas=5
kubectl rollout status deployment nginx
kubectl rollout history deployment nginx
kubectl rollout undo deployment nginx

# Services
kubectl expose deployment nginx --port=80 --type=LoadBalancer
kubectl get services
kubectl describe service nginx

# ConfigMaps and Secrets
kubectl create configmap app-config --from-file=config.properties
kubectl create secret generic db-secret --from-literal=password=secret123
kubectl get configmaps
kubectl get secrets

# Namespaces
kubectl get namespaces
kubectl create namespace production
kubectl config set-context --current --namespace=production

# Apply Configuration
kubectl apply -f deployment.yaml
kubectl delete -f deployment.yaml

# Debugging
kubectl get events
kubectl top nodes
kubectl top pods
```

---

## Summary

### Key Architectural Principles

```ascii
╔════════════════════════════════════════════════════════════╗
║              KUBERNETES CORE PRINCIPLES                    ║
╠════════════════════════════════════════════════════════════╣
║                                                            ║
║  1. Declarative Configuration                              ║
║     State what you want, not how to achieve it            ║
║                                                            ║
║  2. Desired State Reconciliation                           ║
║     Controllers continuously match reality to desired      ║
║                                                            ║
║  3. Self-Healing                                           ║
║     Automatic recovery from failures                       ║
║                                                            ║
║  4. Horizontal Scaling                                     ║
║     Scale by adding more pods, not bigger pods            ║
║                                                            ║
║  5. Service Abstraction                                    ║
║     Stable networking despite pod churn                    ║
║                                                            ║
║  6. Immutable Infrastructure                               ║
║     Replace, don't modify running containers              ║
║                                                            ║
║  7. Microservices-Friendly                                 ║
║     Built for distributed, loosely-coupled systems        ║
║                                                            ║
╚════════════════════════════════════════════════════════════╝
```

### Evolution Journey

```mermaid
timeline
    title Infrastructure Evolution
    2000s : Physical Servers
          : Monolithic Applications
          : Manual Configuration
    2005-2010 : Virtual Machines
              : Mutable Infrastructure
              : Configuration Management (Chef, Puppet)
    2010-2013 : Cloud Computing
              : Infrastructure as Code
              : Google Borg (Internal)
    2013-2015 : Containers (Docker)
              : Immutable Infrastructure
              : Microservices Architecture
    2014 : Kubernetes Born
         : Open-sourced by Google
         : Based on Borg lessons
    2015-Present : Container Orchestration
                 : Cloud-Native Applications
                 : Kubernetes Standard
```

### Why Kubernetes Won

```markmap
# Kubernetes Success Factors
## Technical Excellence
### Battle-tested at Google scale
### Excellent architecture
### Strong API design
### Extensible platform
## Community & Ecosystem
### CNCF governance
### Vendor-neutral
### Large contributor base
### Rich ecosystem (Helm, Istio, etc.)
## Cloud Provider Support
### AWS EKS
### Azure AKS
### Google GKE
### Managed offerings everywhere
## Industry Adoption
### Became de facto standard
### Enterprise support
### Training and certification
### Job market demand
```

### Kubernetes vs Alternatives

|Feature|Kubernetes|Docker Swarm|Nomad|Mesos|
|---|---|---|---|---|
|**Complexity**|High|Low|Medium|High|
|**Scalability**|Excellent|Good|Good|Excellent|
|**Ecosystem**|Largest|Small|Medium|Medium|
|**Learning Curve**|Steep|Gentle|Moderate|Steep|
|**Community**|Massive|Declining|Growing|Declining|
|**Cloud Support**|Universal|Limited|Good|Limited|
|**Best For**|Production|Simple setups|Multi-workload|Big data|
|**Market Share**|Dominant|Legacy|Niche|Niche|

---

# Kuber Cluster

1. KIND Cluster :- K8s in Docker
2. KubeADM:- Multiple E2C instance and combine them to form cluster
3. MiniKube:- Local Host Use 
4. EKS/AKS:- Amazon Kubernetes or Elastic Kubernetes 