---
title:
  "{ Title }":
tags:
  - OS
created:
  "{ date }":
updated:
  "{ date }":
---


## 1. Memory Management Overview

**Definition**: Memory management is the process of controlling and coordinating computer memory, assigning portions called blocks to various running programs to optimize overall system performance.

**Primary Goals**:

- **Allocation**: Efficiently allocate memory to processes
- **Protection**: Prevent processes from accessing each other's memory
- **Sharing**: Allow controlled sharing of memory between processes
- **Relocation**: Load processes at different memory addresses
- **Logical Organization**: Match program structure
- **Physical Organization**: Handle memory hierarchy (cache, RAM, disk)

**Key Concepts**:

- **Logical Address**: Generated by CPU (also called virtual address)
- **Physical Address**: Actual address in RAM
- **Address Binding**: Mapping logical addresses to physical addresses

## 2. Address Binding and Loading

### Address Binding Times

**1. Compile Time**:

- Absolute addresses generated at compile time
- Must know exact memory location beforehand
- Program cannot be relocated
- Rarely used in modern systems

**2. Load Time**:

- Compiler generates relocatable code
- Loader binds addresses when program loads
- Can load at different locations but cannot move during execution

**3. Execution Time (Runtime)**:

- Binding delayed until runtime
- Process can move during execution
- Requires hardware support (MMU)
- Used by modern operating systems

### Loading Process into Main Memory

**Step 1: Compilation**

- Source code → Compiler → Object code (relocatable addresses)
- References are symbolic or relative

**Step 2: Linking**

- **Static Linking**: All library code included in executable at compile time
- **Dynamic Linking**: Libraries loaded at runtime (DLLs, shared objects)
- Linker resolves external references

**Step 3: Loading** The **Loader** performs:

**a) Memory Allocation**:

- OS allocates memory space for the process
- Creates process control block (PCB)
- Allocates space for code, data, stack, heap segments

**b) Loading into Memory**:

- Copies executable from disk to allocated RAM
- May load entire program (contiguous allocation)
- Or load portions as needed (virtual memory)

**c) Address Resolution**:

- Resolves relocatable addresses to actual physical addresses
- Updates address references in loaded code

**d) Initialization**:

- Sets up initial register values (PC, SP, etc.)
- Initializes global variables
- Prepares process for execution

### Loading Strategies

**1. Absolute Loading**

- Program loaded at fixed address
- No relocation needed
- Inflexible, rarely used

**2. Relocatable Loading**

- Loader adjusts addresses based on actual load address
- Uses relocation register
- Single contiguous memory block required

**3. Dynamic Loading**

- Routine loaded only when called
- Reduces initial memory requirements
- Better memory utilization

**4. Overlays**

- Manual programmer technique
- Load only needed sections
- Older technique, replaced by virtual memory

## 3. Memory Allocation Techniques

### Contiguous Memory Allocation

**Fixed Partitioning**:

- Memory divided into fixed-size partitions
- Each process gets one partition
- **Internal fragmentation**: Process smaller than partition wastes space
- Simple but inefficient

**Dynamic Partitioning**:

- Partitions created dynamically based on process size
- **External fragmentation**: Small holes between partitions
- Allocation algorithms:
    - **First Fit**: Allocate first sufficient hole (fast)
    - **Best Fit**: Allocate smallest sufficient hole (minimizes waste but slow)
    - **Worst Fit**: Allocate largest hole (leaves larger remaining holes)

**Compaction**:

- Relocate processes to combine free holes
- Expensive operation, requires relocation capability

### Non-Contiguous Memory Allocation

Allows process to occupy multiple non-adjacent memory locations:

- **Paging**: Fixed-size blocks
- **Segmentation**: Variable-size logical units
- **Segmented Paging**: Combination of both

## 4. Virtual Memory - Core Theory

**Definition**: Virtual memory is a memory management capability that provides an "idealized abstraction of the storage resources" that gives each process the illusion of having its own private, large, contiguous address space.

**Fundamental Principle**: Separation of logical memory (what the process sees) from physical memory (actual RAM).

**Why Virtual Memory?**

- Programs larger than physical RAM can execute
- More processes can reside in memory simultaneously
- Efficient memory utilization through demand loading
- Process isolation and security
- Simplified memory management for programmers

**Key Requirements**:

- **Hardware Support**: Memory Management Unit (MMU) for address translation
- **OS Support**: Page fault handling, page replacement algorithms
- **Secondary Storage**: Disk space for swapping

### Memory Management Unit (MMU)

**Function**: Translates virtual addresses to physical addresses

**Components**:

- Translation Lookaside Buffer (TLB)
- Page table registers
- Control logic for address translation

**Translation Process**:

1. CPU generates virtual address
2. MMU checks TLB for translation
3. If TLB hit: Use cached translation
4. If TLB miss: Access page table in memory
5. If page in memory: Load translation into TLB
6. If page fault: Trigger OS page fault handler

## 5. Paging in Detail

### Structure and Components

**Page**: Fixed-size block of virtual memory (typically 4KB, 8KB)

**Frame**: Fixed-size block of physical memory (same size as page)

**Page Table**: Data structure mapping virtual pages to physical frames

**Virtual Address Format**:

```
| Page Number (p) | Page Offset (d) |
```

- **Page Number**: Index into page table
- **Page Offset**: Position within page/frame (remains unchanged in translation)

**Page Table Entry (PTE)**:

- **Frame Number**: Physical frame location
- **Valid/Invalid Bit**: Is page in memory?
- **Protection Bits**: Read/Write/Execute permissions
- **Reference Bit**: Recently accessed? (for LRU approximation)
- **Modified/Dirty Bit**: Has page been written to?
- **Caching Disabled**: For I/O memory

### Page Table Implementation

**1. Simple/Single-Level Page Table**

- Array indexed by page number
- One entry per virtual page
- **Problem**: Very large for big address spaces
    - Example: 32-bit address, 4KB pages → 2²⁰ pages = 1M entries
    - Per process overhead is huge

**2. Hierarchical/Multi-Level Paging**

**Two-Level Paging**:

```
Virtual Address: | P1 | P2 | Offset |
```

- P1 indexes outer page table
- P2 indexes inner page table
- Saves space when address space sparsely populated
- Example: x86 uses two-level paging

**Three-Level Paging**:

- Used in 64-bit systems
- Example: P1 → P2 → P3 → Offset
- Further reduces memory overhead

**Four-Level Paging**:

- Used in modern 64-bit systems (x86-64)
- Can handle huge virtual address spaces (48 bits)

**3. Hashed Page Tables**

- Virtual page number hashed to page table
- Good for sparse address spaces
- Each entry contains chain of pages with same hash

**4. Inverted Page Tables**

- One entry per physical frame (not per virtual page)
- Entry contains: Process ID + Virtual page number
- Reduces memory overhead significantly
- **Drawback**: Slower lookup (must search or use hash)
- Used in: PowerPC, IBM System/38

### Translation Lookaside Buffer (TLB)

**Purpose**: Cache for page table entries to speed up translation

**Characteristics**:

- Small, fast associative memory (16-1024 entries)
- Contains recent virtual-to-physical mappings
- Hit ratio typically 98-99%

**TLB Operation**:

1. Virtual address presented
2. TLB searched in parallel (associative lookup)
3. **TLB Hit**: Physical address retrieved immediately (fast)
4. **TLB Miss**: Access page table in memory (slow), load entry into TLB

**Effective Access Time Calculation**:

```
EAT = (TLB_hit_ratio × TLB_access_time) + 
      (TLB_miss_ratio × (TLB_access_time + Memory_access_time + Page_table_access_time))
```

**TLB Management**:

- **Context Switch**: TLB must be flushed or tagged with process ID (ASID)
- **Replacement**: LRU or random when TLB full

### Page Faults

**Definition**: Exception raised when process accesses page not in physical memory

**Page Fault Handling Steps**:

1. CPU generates address for page not in memory
2. MMU detects invalid page table entry
3. Trap to OS (page fault interrupt)
4. OS checks if reference is valid (in process address space)
5. If invalid: Terminate process (segmentation fault)
6. If valid: Find free frame or select victim page
7. If victim is dirty: Write to disk (swap out)
8. Read required page from disk to free frame (swap in)
9. Update page table with new frame number
10. Set valid bit
11. Restart instruction that caused fault

**Types of Page Faults**:

- **Minor (Soft) Fault**: Page in memory but not in page table (e.g., shared page)
- **Major (Hard) Fault**: Page must be loaded from disk
- **Invalid Fault**: Access to invalid address (error)

**Performance Impact**:

- Page fault service time: 1-10 milliseconds
- Normal memory access: 10-200 nanoseconds
- Page faults are extremely expensive (1,000,000× slower)

### Page Replacement Algorithms

When memory is full and page fault occurs, must select victim page.

**1. FIFO (First-In-First-Out)**

- Replace oldest page in memory
- **Advantage**: Simple to implement (queue)
- **Disadvantage**: May remove frequently used pages
- **Belady's Anomaly**: More frames can increase page faults

**2. Optimal (OPT)**

- Replace page that won't be used for longest time
- **Advantage**: Lowest possible page fault rate
- **Disadvantage**: Impossible to implement (requires future knowledge)
- Used as theoretical benchmark

**3. LRU (Least Recently Used)**

- Replace page not used for longest time (past)
- **Advantage**: Good approximation of optimal, respects locality
- **Disadvantage**: Expensive to implement precisely
- **Implementation**:
    - **Counter**: Timestamp on each access (high overhead)
    - **Stack**: Doubly linked list (expensive updates)

**4. LRU Approximation Algorithms**

**a) Second Chance (Clock Algorithm)**:

- FIFO with reference bit
- Check reference bit when page selected:
    - If bit = 1: Clear bit, give second chance, check next
    - If bit = 0: Replace page
- Circular queue with pointer (clock hand)

**b) Enhanced Second Chance**:

- Uses both reference and modified bits
- Four classes (prefer in order):
    1. (0,0): Not recently used, not modified (best)
    2. (0,1): Not recently used, modified
    3. (1,0): Recently used, not modified
    4. (1,1): Recently used, modified (worst)

**5. Counting Algorithms**

**a) LFU (Least Frequently Used)**:

- Replace page with smallest access count
- Problem: Early pages may have high count

**b) MFU (Most Frequently Used)**:

- Replace page with highest access count
- Assumption: Low count means recently loaded

**6. Page Buffering**

- Keep pool of free frames
- Keep replaced pages in memory temporarily
- If referenced again before reuse: Quick recovery

### Paging Problems and Solutions

**Problem 1: Large Page Tables**

_Solutions_:

- **Hierarchical Paging**: Multi-level page tables
- **Hashed Page Tables**: Hash virtual page number
- **Inverted Page Tables**: One entry per frame
- **Larger Page Sizes**: Fewer pages (increases internal fragmentation)

**Problem 2: Slow Translation**

_Solutions_:

- **TLB**: Cache recent translations
- **Larger Pages**: Fewer translations needed
- **Superpages/Huge Pages**: 2MB or 1GB pages for databases

**Problem 3: Thrashing**

**Definition**: System spends more time paging than executing

**Causes**:

- Too many processes in memory
- Insufficient frames per process
- High page fault rate

_Solutions_:

**a) Working Set Model**:

- Working set: Pages referenced in recent time window (Δ)
- Keep entire working set in memory
- If insufficient frames: Suspend process

**b) Page Fault Frequency (PFF)**:

- Monitor page fault rate per process
- If rate too high: Allocate more frames
- If rate too low: Remove frames
- Thrashing occurs when cannot satisfy all PFF requirements

**c) Load Control**:

- Reduce degree of multiprogramming
- Suspend some processes (swap out)
- Use medium-term scheduler

**Problem 4: Internal Fragmentation**

- Average waste: 0.5 pages per process
- With 4KB pages: Average 2KB wasted

_Mitigation_:

- Smaller pages (increases overhead)
- Accept as trade-off

## 6. Segmentation in Detail

### Structure and Concepts

**Segment**: Variable-sized logical unit representing program structure

**Common Segments**:

- Code segment (text)
- Data segment (global variables)
- Stack segment
- Heap segment
- Shared library segments

**Segment Table**: Maps segments to physical memory

**Virtual Address Format**:

```
| Segment Number (s) | Offset (d) |
```

**Segment Table Entry**:

- **Base Address**: Starting physical address of segment
- **Limit/Length**: Size of segment
- **Protection Bits**: Read/Write/Execute permissions
- **Growth Direction**: For stack (grows downward)
- **Valid Bit**: Is segment in memory?

### Address Translation

**Steps**:

1. Extract segment number and offset from virtual address
2. Check segment number < segment table length
3. Access segment table entry
4. Check offset < segment limit (protection)
5. Calculate physical address: base + offset
6. Access memory at physical address

**Protection Checking**:

- Offset must be less than segment limit
- If violated: Segmentation fault (trap to OS)
- Allows natural protection boundaries

### Advantages of Segmentation

**1. Logical Organization**:

- Matches program structure naturally
- Programmer views as collection of segments

**2. Protection**:

- Different segments can have different protections
- Code: Read + Execute (no write)
- Data: Read + Write (no execute)
- Stack: Read + Write + Execute (sometimes)

**3. Sharing**:

- Multiple processes can share segments
- Shared libraries (libc, system libraries)
- Code sharing between processes
- Each process has own data segment but shares code

**4. Dynamic Growth**:

- Segments can grow (heap, stack)
- OS can adjust segment sizes

**5. No Internal Fragmentation**:

- Segments sized exactly for content

### Segmentation Problems and Solutions

**Problem 1: External Fragmentation**

**Cause**: Variable-sized segments create holes in memory

_Solutions_:

**a) Compaction**:

- Relocate segments to consolidate free space
- Requires all programs to be relocatable
- Expensive operation (stop all processes, copy memory)
- Rarely done in practice

**b) Allocation Strategies**:

- **First Fit**: Allocate first sufficient hole (fast)
- **Best Fit**: Smallest sufficient hole (minimizes waste)
- **Worst Fit**: Largest hole (leaves larger remaining holes)
- **Buddy System**: Power-of-2 sizes for easier management

**c) Combine with Paging** (see Section 7)

**Problem 2: Large Segments**

**Issue**: Large segment may not fit in available holes

_Solutions_:

- Break into smaller segments
- Use segmentation with paging
- Swap out other segments

**Problem 3: Segment Growth**

**Issue**: Growing segment (heap/stack) may lack adjacent space

_Solutions_:

- Pre-allocate extra space
- Relocate segment when growth needed
- Use segmentation with paging (segment spans multiple pages)

**Problem 4: Fragmentation Over Time**

As processes create/destroy segments, memory becomes fragmented

_Solutions_:

- Periodic compaction
- Careful allocation strategies
- Segmentation with paging

## 7. Segmentation with Paging (Hybrid Approach)

### Combined Architecture

**Purpose**: Get benefits of both techniques while avoiding their drawbacks

**Structure**:

- **Segmentation**: Logical view, protection, sharing
- **Paging**: Physical allocation, no external fragmentation

**How It Works**:

- Each segment is divided into pages
- Segment table points to page tables (not physical memory)
- Each segment has its own page table

### Address Translation

**Virtual Address Format**:

```
| Segment Number | Page Number | Page Offset |
```

**Translation Steps**:

1. Extract segment number from virtual address
2. Access segment table entry
3. Check segment limit (protection)
4. Get page table base address from segment table
5. Extract page number from virtual address
6. Access page table (for that segment)
7. Get frame number from page table entry
8. Extract page offset
9. Physical address = Frame number + Page offset

**Example (Intel x86)**:

- Segment table (GDT/LDT) → Page directory → Page table → Physical frame
- Two-level paging within each segment

### Advantages

1. **Logical Organization**: Segments reflect program structure
2. **No External Fragmentation**: Pages eliminate this issue
3. **Protection**: Per-segment protection
4. **Sharing**: Segments can be shared between processes
5. **Dynamic Growth**: Segments can grow by allocating more pages
6. **No Large Contiguous Allocation**: Only individual pages need free frames

### Disadvantages

1. **Complexity**: More complex address translation
2. **Multiple Table Lookups**: Slower without TLB
3. **Memory Overhead**: Both segment and page tables
4. **TLB Management**: More complex caching

## 8. Demand Paging

**Definition**: Pages loaded only when demanded (accessed), not in advance

**Principle**: Lazy loading based on locality of reference

**Process**:

1. Process starts with no pages in memory
2. As instructions execute, page faults occur
3. OS loads pages on demand
4. Eventually working set is in memory

**Advantages**:

- Faster program startup
- Less memory used initially
- More processes in memory simultaneously
- Programs larger than physical memory can run

**Pure Demand Paging**: Never bring page into memory until required

**Prepaging**: Load multiple pages anticipating need (reduces page faults)

### Demand Paging Performance

**Factors Affecting Performance**:

- **Page fault rate** (p): 0 ≤ p ≤ 1
- **Memory access time** (ma): 100-200 nanoseconds
- **Page fault service time** (pf): 1-10 milliseconds

**Effective Access Time**:

```
EAT = (1 - p) × ma + p × pf
```

**Example**:

- ma = 200 ns
- pf = 8 ms = 8,000,000 ns
- p = 0.001 (1 fault per 1000 accesses)

```
EAT = 0.999 × 200 + 0.001 × 8,000,000
    = 199.8 + 8,000
    = 8,199.8 ns ≈ 8.2 μs
```

**Slowdown factor**: 8,200/200 = 41× slower

**To keep degradation < 10%**:

```
200 × 1.1 = (1 - p) × 200 + p × 8,000,000
220 = 200 - 200p + 8,000,000p
20 = 7,999,800p
p < 0.0000025 (1 fault per 400,000 accesses)
```

## 9. Memory Management in Modern Systems

### Copy-on-Write (COW)

**Concept**: Share pages between processes until one modifies it

**Usage**: Fork() system call

1. Parent and child initially share same physical pages
2. Pages marked read-only with COW bit set
3. When either process writes: Page fault occurs
4. OS creates copy of page for writing process
5. Original remains with other process

**Benefits**:

- Fast process creation
- Memory efficient
- Only modified pages are copied

### Memory-Mapped Files

**Concept**: Map file directly into virtual address space

**Process**:

1. File mapped to portion of virtual address space
2. Pages loaded on demand (page faults)
3. File I/O becomes memory access
4. Modified pages written back to file

**Benefits**:

- Simplified file I/O
- Efficient for large files
- Shared memory between processes

### Kernel Memory Allocation

**Special Requirements**:

- Contiguous physical memory (DMA devices)
- Various sizes needed
- Must be fast

**Buddy System**:

- Memory allocated in power-of-2 sizes
- Split and coalesce for efficient management
- Example: 64KB request from 1MB pool
    - 1MB → 512KB + 512KB
    - 512KB → 256KB + 256KB
    - 256KB → 128KB + 128KB
    - 128KB → 64KB + 64KB (allocate one 64KB block)

**Slab Allocator**:

- Cache of objects for kernel structures
- Reduces allocation/deallocation overhead
- Pre-initialized objects ready for use

## 10. Key Interview Points Summary

**Memory Management Essentials**:

- Address binding (compile, load, execution time)
- Logical vs physical addresses
- MMU role in translation
- Loading process: allocation → loading → relocation → initialization

**Virtual Memory Core**:

- Separation of logical and physical memory
- Demand paging for efficiency
- Page tables and TLB for translation

**Paging**:

- Fixed-size pages eliminate external fragmentation
- Page table overhead and multi-level solutions
- TLB critical for performance
- Page replacement algorithms (FIFO, LRU, Clock)
- Thrashing and working set model

**Segmentation**:

- Logical program organization
- Variable sizes cause external fragmentation
- Natural protection and sharing boundaries
- Compaction needed for fragmentation

**Hybrid Approach**:

- Segmentation with paging combines benefits
- Segments for logic, pages for physical allocation
- More complex but eliminates both fragmentation types

**Performance Considerations**:

- Page fault overhead enormous
- TLB hit ratio critical
- Locality of reference key to success
- Balance between page size and table overhead

This comprehensive theory covers all aspects interviewers typically ask about memory management and virtual memory systems.